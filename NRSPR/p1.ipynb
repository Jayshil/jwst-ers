{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NIRSpec Prism data analysis I. Finding calibrated data\n",
    "\n",
    "In this notebook, we will produce corrected data from `uncal` JWST data.\n",
    "\n",
    "For stage 1 processing, we will mainly use the `jwst` pipeline with some modifications. The main modification is the use of `refpix` step. This dataset does not have reference pixels on three edges of the detector; some earlier works with ERS data have shown that background subtraction at group level works more efficiently than when it is done at integration level. Therefore, we will perform a group level background subtraction using column-by-column background subtraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from jwst import datamodels\n",
    "from jwst.pipeline import calwebb_detector1\n",
    "from stark import reduce\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import utils\n",
    "from stark import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = '/'.join(os.getcwd().split('/')[:-1])\n",
    "pin = p1 + '/Data/NRSPR'\n",
    "pout = p1 + '/NRSPR/Outputs/Tests'\n",
    "\n",
    "fname = 'jw01366004001_04101_00001-seg004_nrs1_uncal.fits'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calwebb detector 1 processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncal = datamodels.RampModel(pin + '/' + fname)\n",
    "nint = np.random.randint(0, uncal.data.shape[0])\n",
    "\n",
    "print(uncal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meaning that the data has 3200 total integration and 5 groups per integrations. An individual frame has 32 by 512 pixels (32 rows and 512 columns). Let's first visualise the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "im = plt.imshow(uncal.data[nint,-1,:,:], interpolation='none')\n",
    "im.set_clim([0,2e4])\n",
    "plt.title('Example data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform `groupscale`, `dq`, `saturation` and `superbias` steps manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupscale_results = calwebb_detector1.group_scale_step.GroupScaleStep.call(uncal, save_results=False)\n",
    "dq_results = calwebb_detector1.dq_init_step.DQInitStep.call(groupscale_results, save_results=False)\n",
    "saturation_results = calwebb_detector1.saturation_step.SaturationStep.call(dq_results, save_results=False)\n",
    "superbias_results = calwebb_detector1.superbias_step.SuperBiasStep.call(saturation_results, save_results=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the counts in one of the rows,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(superbias_results.data[nint,-1,25,:])\n",
    "#plt.ylim([10250,12750])\n",
    "plt.xlim([0, 512])\n",
    "plt.xlabel('Column number')\n",
    "plt.title('Flux measured in the last group of an arbitrary integration in an arbitrary row (for 2nd and 3rd amplifier)')\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(superbias_results.data[nint,-1,25,:])\n",
    "#plt.ylim([10250,12250])\n",
    "plt.xlim([200,300])\n",
    "plt.xlabel('Column number')\n",
    "plt.title('Same as above, but zoom-in at columns b/w 600-700 to see the odd-even effect.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't think there is any odd-even effect present here! But let's go ahead with the `refpix` correction step anyway and then we will perfrom background subtraction. Since this data doesn't have any real reference pixels, the pipeline will asign top and bottom five pixels as reference pixels and perform the `refpix` step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refpix_results = calwebb_detector1.refpix_step.RefPixStep.call(superbias_results, save_results=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, did it properly correct for odd-even noise?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(refpix_results.data[nint,-1,25,:])\n",
    "#plt.ylim([10250,12750])\n",
    "plt.xlim([0, 512])\n",
    "plt.xlabel('Column number')\n",
    "plt.title('Flux measured in the last group of an arbitrary integration in an arbitrary row (for 2nd and 3rd amplifier)')\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(refpix_results.data[nint,-1,25,:])\n",
    "#plt.ylim([10250,12250])\n",
    "plt.xlim([200,300])\n",
    "plt.xlabel('Column number')\n",
    "plt.title('Same as above, but zoom-in at columns b/w 600-700 to see the odd-even effect.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we perform the background subtraction -- we first design a mask for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "im = plt.imshow(refpix_results.data[nint,-1,:,:], interpolation='none')\n",
    "im.set_clim([-100,2e4])\n",
    "plt.axvline(15, color='cyan')\n",
    "plt.axvline(475, color='cyan')\n",
    "plt.axhline(8, color='cyan')\n",
    "plt.axhline(22, color='cyan')\n",
    "plt.title('Example data frame')\n",
    "plt.show()\n",
    "\n",
    "m1 = np.ones(refpix_results.data[nint,-1,:,:].shape)\n",
    "m1[8:22,15:475] = 0.\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.imshow(m1, interpolation='none')\n",
    "plt.title('Mask (M1)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will perform column-by-column background subtraction. For each column, we will estimate the background by computing median counts along each column. We finally subtract the estimated background from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Column-by-column background subtraction on group level\n",
    "sub_bkg_col = np.ones(refpix_results.data.shape)\n",
    "for integration in tqdm(range(refpix_results.data.shape[0])):\n",
    "    for group in range(refpix_results.data.shape[1]):\n",
    "        refpix_results.data[integration, group, :, :], sub_bkg_col[integration, group, :, :] =\\\n",
    "            reduce.col_by_col_bkg_sub(frame=refpix_results.data[integration, group, :, :], mask=m1)\n",
    "        \n",
    "# Now, let's plot the subtracted background:\n",
    "plt.figure(figsize=(15,5))\n",
    "im = plt.imshow(sub_bkg_col[nint,-1,:,:], interpolation='none')\n",
    "plt.title('Subtracted background from last group of {:d}th integration'.format(nint))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(refpix_results.data[nint,-1,25,:])\n",
    "plt.axhline(0., color='k', ls='--')\n",
    "plt.ylim([-100,100])\n",
    "plt.xlim([0,512])\n",
    "plt.title('Flux measured in the last group of an arbitrary integration in an arbitrary row')\n",
    "plt.xlabel('Column Number')\n",
    "plt.ylabel('Counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! This is a good background subtraction!\n",
    "\n",
    "Now again continuing with the rest of the steps from the pipeline. We won't perform the jump correction step here, because that step is known to flag many good points as outliers for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linearity_results = calwebb_detector1.linearity_step.LinearityStep.call(refpix_results,\n",
    "                                                                        save_results=False)\n",
    "darkcurrent_results = calwebb_detector1.dark_current_step.DarkCurrentStep.call(linearity_results,\n",
    "                                                                            save_results=False)\n",
    "rampfitting_results = calwebb_detector1.ramp_fit_step.RampFitStep.call(darkcurrent_results, maximum_cores='all',\\\n",
    "                                                                       output_dir=pout, save_results=True)\n",
    "gainscale_results = calwebb_detector1.gain_scale_step.GainScaleStep.call(rampfitting_results[1],\\\n",
    "                                                                         output_dir=pout, save_results=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dq = rampfitting_results[1].dq\n",
    "mask = np.ones(dq.shape)\n",
    "mask[dq > 0] = 0.\n",
    "\n",
    "print('Total per cent of masked points: {:.4f} %'.format(100 * (1 - np.sum(mask) / (mask.shape[0] * mask.shape[1] * mask.shape[2]))))\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.imshow(mask[nint,:,:], interpolation='none')\n",
    "plt.title('Bad-pixels for an arbitrary integration')\n",
    "#plt.xlim([512,1536])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so there are about ~3% of pixels are `bad' -- most of them are because of saturation. For example -- the pixels near 100th column are saturated -- so, we will mask them during the spectral extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.imshow(gainscale_results.data[nint,:,:], interpolation='none')\n",
    "plt.title('Example data frame')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(gainscale_results.data[nint,25,:])\n",
    "#plt.axvline(1536, color='k')\n",
    "plt.axhline(0., color='k')\n",
    "plt.xlabel('Column number')\n",
    "plt.ylabel('Counts')\n",
    "plt.title('Flux level in an arbitrary row of an arbitrary integration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(gainscale_results.data[nint-10,:,50])\n",
    "plt.ylim([-50,1000])\n",
    "plt.axhline(0., color='k')\n",
    "plt.xlabel('Row number')\n",
    "plt.ylabel('Counts')\n",
    "plt.title('Flux level in an arbitrary column of an arbitrary integration')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe, we need to perform another column-by-column background subtraction at the column level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_bjd = uncal.int_times['int_mid_BJD_TDB']\n",
    "\n",
    "print('>>>> --- Correcting errorbars (for zeros and NaNs)...')\n",
    "## Correct errorbars\n",
    "med_err = np.nanmedian(gainscale_results.err.flatten())\n",
    "## Changing Nan's and zeros in error array with median error\n",
    "corr_err1 = np.copy(gainscale_results.err)\n",
    "corr_err2 = np.where(gainscale_results.err != 0., corr_err1, med_err)                     # Replacing error == 0 with median error\n",
    "corrected_errs = np.where(np.isnan(gainscale_results.err) != True, corr_err2, med_err)    # Replacing error == Nan with median error\n",
    "print('>>>> --- Done!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And creating a bad-pixel map (this is in addition to the bad-pixel map we already have -- the idea is to include all those pixels with 0 or NaN errors in bad pixel map. Additionally, we will manually identify cosmic rays by comparing median frame with each frame and add those pixels in the bad-pixel map),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('>>>> --- Creating a bad-pixel map...')\n",
    "## Making a bad-pixel map\n",
    "mask_bp1 = np.ones(gainscale_results.data.shape)\n",
    "mask_bp2 = np.where(gainscale_results.err != 0., mask_bp1, 0.)                 # This will place 0 in mask where errorbar == 0\n",
    "mask_bp3 = np.where(np.isnan(gainscale_results.err) != True, mask_bp2, 0.)     # This will place 0 in mask where errorbar is Nan\n",
    "#mask_badpix = np.where(dq == 0., mask_bp3, 0.)                               # This will place 0 in mask where darkdq != 0\n",
    "mask_badpix = mask * mask_bp3                                                 # Adding those pixels which are identified as bad by the pipeline (and hence 0)\n",
    "\n",
    "## Mask with cosmic rays\n",
    "### Essentially this mask will add 0s in the places of bad pixels...\n",
    "mask_bcr = utils.identify_crays(gainscale_results.data, mask_badpix)\n",
    "print('>>>> --- Done!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total per cent of masked points: {:.4f} %'.format(100 * (1 - np.sum(mask_bcr) / (mask_bcr.shape[0] * mask_bcr.shape[1] * mask_bcr.shape[2]))))\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.imshow(mask_bcr[nint,:,:], interpolation='none')\n",
    "plt.title('Bad-pixels for an arbitrary integration')\n",
    "#plt.xlim([512,1536])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And correct the data (meaning replacing NaN values in data-frame with average of neighbouring pixels),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('>>>> --- Correcting data...')\n",
    "corrected_data = np.copy(gainscale_results.data)\n",
    "corrected_data[mask_bcr == 0] = np.nan\n",
    "for i in range(corrected_data.shape[0]):\n",
    "    corrected_data[i,:,:] = utils.replace_nan(corrected_data[i,:,:])\n",
    "print('>>>> --- Done!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, finally, we will perform a row-by-row background subtraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_data_bkg = np.ones(corrected_data.shape)\n",
    "for i in tqdm(range(corrected_data.shape[0])):\n",
    "    corrected_data_bkg[i,:,:], _ = reduce.col_by_col_bkg_sub(corrected_data[i,:,:], mask=m1*mask_bcr[i,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(corrected_data_bkg[nint,25,:])\n",
    "plt.axhline(0., color='k')\n",
    "plt.title('Flux measured in an arbitrary row of an arbitrary integration')\n",
    "plt.xlabel('Column number')\n",
    "plt.ylabel('Counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(corrected_data_bkg[nint,:,100])\n",
    "plt.ylim([-50,1000])\n",
    "plt.axhline(0., color='k')\n",
    "plt.xlabel('Row number')\n",
    "plt.ylabel('Counts')\n",
    "plt.title('Flux level in an arbitrary column of an arbitrary integration')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the data,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(pout + '/Corrected_data.npy', corrected_data_bkg)\n",
    "np.save(pout + '/Corrected_err.npy', corrected_errs)\n",
    "np.save(pout + '/Mask_bcr.npy', mask_bcr)\n",
    "np.save(pout + '/times.npy', times_bjd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jwst",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
